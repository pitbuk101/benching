client_name: "default_run" # Default client name, can be overridden by API requests

paths:
  # Temporary local directory
  base_temp_dir: "./data/temp/"
  log_dir: "./logs/"

# S3 Configuration
s3:
  input_bucket: ${oc.env:INPUT_S3_BUCKET}
  benchmark_input_bucket: ${oc.env:BENCHMARK_INPUT_S3_BUCKET}

# Snowflake Configuration
snowflake:
  user_env_var: ${oc.env:EU_SF_USERNAME}
  password_env_var: ${oc.env:EU_SF_PASSWORD}
  account_env_var: ${oc.env:EU_SF_ACCOUNT}
  warehouse_env_var: ${oc.env:EU_SF_WAREHOUSE}
  database_env_var: ${oc.env:EU_IDP_SF_DATABASE}
  role_env_var: ${oc.env:EU_SF_ROLE}

postgres:
  host_env_var: ${oc.env:PGHOST}
  port_env_var: ${oc.env:PGPORT} 
  database_env_var: ${oc.env:PGDATABASE}
  user_env_var: ${oc.env:PGUSER}
  password_env_var: ${oc.env:PGPASSWORD}
  schema_env_var: ${oc.env:PGSCHEMA}
  ssl_mode: "require"

# LLM Configuration
llm:
  model: "gpt-4o"
  temperature: 0.0
  batch_size: 10
  api_key_env_var: ${oc.env:OPENAI_API_KEY}
  base_url_env_var: ${oc.env:OPENAI_BASE_URL}
  timeout_seconds: 120
  max_retries: 3
  max_workers_normalization: 4


# Logging Configuration
logging:
  level: "INFO"
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  date_format: "%Y-%m-%d %H:%M:%S"
  log_file_enabled: true

# Input Data Configuration
input_data:
  source_text_column: ['Item Description', 'Description', 'Desc', 'Invoice description', 'Material']
  
# Normalization Stage Configuration
normalization:
  input_text_column_for_llm: "description"
  llm_prompt_key: "generic_normalization_prompt"
  llm_output_columns: ["Type", "Extracted_Quantity", "Normalized Description", "B2B Query","Measure 1","Measure 2","Measure 3"]
  pre_llm_operations:
    - type: "strip_column"
      column: "description"
    - type: "clean_text_basic"
      column: "description"